{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ML Web App using Flask"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Importing the necessary libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.linear_model import LinearRegression\n",
                "import joblib\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
                "from sklearn.linear_model import Ridge\n",
                "from sklearn.model_selection import GridSearchCV\n",
                "import os"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load and Prepare the Dataset"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "> In this step, I loaded a historical dataset of Amazon (AMZN) stock prices from Yahoo Finance. \n",
                "- I downloaded this dataset from this URL: ***https://www.kaggle.com/datasets/adilshamim8/amazon-stock-price-history***\n",
                "- This dataset contains daily trading data with columns such as Date, Open, High, Low, Close, and Volume. \n",
                "- These are common features in time series financial data and will help us analyze the stock's historical behavior and eventually forecast its future prices. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "         Date     Close      High       Low      Open      Volume\n",
                        "0  1997-05-15  0.097917  0.125000  0.096354  0.121875  1443120000\n",
                        "1  1997-05-16  0.086458  0.098958  0.085417  0.098438   294000000\n",
                        "2  1997-05-19  0.085417  0.088542  0.081250  0.088021   122136000\n",
                        "3  1997-05-20  0.081771  0.087500  0.081771  0.086458   109344000\n",
                        "4  1997-05-21  0.071354  0.082292  0.068750  0.081771   377064000\n",
                        "<class 'pandas.core.frame.DataFrame'>\n",
                        "RangeIndex: 7086 entries, 0 to 7085\n",
                        "Data columns (total 6 columns):\n",
                        " #   Column  Non-Null Count  Dtype  \n",
                        "---  ------  --------------  -----  \n",
                        " 0   Date    7086 non-null   object \n",
                        " 1   Close   7086 non-null   float64\n",
                        " 2   High    7086 non-null   float64\n",
                        " 3   Low     7086 non-null   float64\n",
                        " 4   Open    7086 non-null   float64\n",
                        " 5   Volume  7086 non-null   int64  \n",
                        "dtypes: float64(4), int64(1), object(1)\n",
                        "memory usage: 332.3+ KB\n"
                    ]
                }
            ],
            "source": [
                "# Load dataset\n",
                "df = pd.read_csv(\"/workspaces/ginotomasd-mlwebappflask/data/Amazon_stock_data.csv\")\n",
                "\n",
                "# Show first few records to understand the structure\n",
                "print(df.head())\n",
                "\n",
                "# Check dataset info to ensure no missing values and understand data types\n",
                "df.info()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "> In this step, I loaded the historical stock price dataset for Amazon, which includes 7,086 trading days. Each row corresponds to a single day of trading data, starting from May 15, 1997. The dataset contains the following columns:\n",
                "\n",
                "- **Date**: The trading day (currently as a string).\n",
                "- **Open**: The price at which the stock opened.\n",
                "- **High**: The highest price reached that day.\n",
                "- **Low**: The lowest price reached that day.\n",
                "- **Close**: The price at market close.\n",
                "- **Volume**: The number of shares traded.\n",
                "\n",
                "> All numeric values are in float format except for Volume (integer), and the Date column will later be converted to datetime format for time series processing. This dataset looks clean, complete, and ready for further exploration.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Preprocessing the Data"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "> In this step, I’m preparing the dataset for modeling. Since this is a time series forecasting task, I need to make sure the data is sorted by date and that the `Date` column is properly converted to datetime format.\n",
                "\n",
                "> I’ll also set it as the index of the DataFrame for easier manipulation.\n",
                "\n",
                "- To make a simple but effective model, I’ll try to predict the next day’s **Close** price using today's features like `Open`, `High`, `Low`, `Close`, and `Volume`. \n",
                "\n",
                "- To do that, I’ll shift the `Close` column one step backward to become our target variable (`Next_Close`). \n",
                "-Then, I’ll drop any resulting `NaN` values after the shift and split the data into train and test sets in chronological order to avoid data leakage.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "((5668, 5), (1417, 5), (5668,), (1417,))"
                        ]
                    },
                    "execution_count": 7,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Convert 'Date' to datetime and sort by date\n",
                "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
                "df = df.sort_values(\"Date\")\n",
                "\n",
                "# Set the date as index (optional but useful)\n",
                "df.set_index(\"Date\", inplace=True)\n",
                "\n",
                "# Create the target variable: next day's close price\n",
                "df[\"Next_Close\"] = df[\"Close\"].shift(-1)\n",
                "\n",
                "# Drop the last row (NaN in target)\n",
                "df.dropna(inplace=True)\n",
                "\n",
                "# Define features and target\n",
                "X = df[[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]]\n",
                "y = df[\"Next_Close\"]\n",
                "\n",
                "# Split into train and test sets (80% train, 20% test), keeping time order\n",
                "split_index = int(len(df) * 0.8)\n",
                "X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]\n",
                "y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]\n",
                "\n",
                "# Confirm the shapes\n",
                "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "> After preparing the dataset, I ended up with 5 features: `Open`, `High`, `Low`, `Close`, and `Volume`. I used these to predict the next day’s `Close` price by shifting that column one day backward.\n",
                "\n",
                "> Then, I split the data into training and testing sets, keeping the time order to avoid leakage (no random shuffling). The result is:\n",
                "\n",
                "- `X_train`: 5,668 rows × 5 features  \n",
                "- `X_test`: 1,417 rows × 5 features  \n",
                "- `y_train`: 5,668 target values  \n",
                "- `y_test`: 1,417 target values\n",
                "\n",
                "> Now I’m ready to train a regression model to forecast Amazon’s next-day closing stock price.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Model Development"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "> To predict Amazon's next-day closing price, I decided to start with a simple Linear Regression model. \n",
                "- Linear regression is a great first step because it's easy to interpret and surprisingly effective for time series with strong trends or correlations.\n",
                "\n",
                "> I’ll fit the model using the training data and evaluate it using common regression metrics on the test set: MAE, MSE, and R². \n",
                "- These metrics help me understand how far off my predictions are on average, and how well the model explains the variance in the data.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Mean Absolute Error (MAE): 2.3557\n",
                        "Mean Squared Error (MSE): 10.5792\n",
                        "R-squared (R²): 0.9921\n"
                    ]
                }
            ],
            "source": [
                "# Create and train the model\n",
                "model = LinearRegression()\n",
                "model.fit(X_train, y_train)\n",
                "\n",
                "# Make predictions\n",
                "y_pred = model.predict(X_test)\n",
                "\n",
                "# Evaluate the model\n",
                "mae = mean_absolute_error(y_test, y_pred)\n",
                "mse = mean_squared_error(y_test, y_pred)\n",
                "r2 = r2_score(y_test, y_pred)\n",
                "\n",
                "# Print the evaluation results\n",
                "print(\"Mean Absolute Error (MAE):\", round(mae, 4))\n",
                "print(\"Mean Squared Error (MSE):\", round(mse, 4))\n",
                "print(\"R-squared (R²):\", round(r2, 4))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "> After training my Linear Regression model on the historical Amazon stock data, I evaluated its performance using the test set. The results look decent, but I don't expect anything out of the ordinary:\n",
                "\n",
                "- **Mean Absolute Error (MAE): 2.36** – On average, my predictions are off by about $2.36, which is relatively low considering the range of Amazon's stock prices.\n",
                "- **Mean Squared Error (MSE): 10.58** – Squaring the errors penalizes larger mistakes, and this value confirms that my model is fairly consistent.\n",
                "- **R-squared (R²): 0.9921** – This means that 99.21% of the variation in the stock prices is explained by the model. That's an excellent score, suggesting a strong correlation between the input features and the next day’s closing price.\n",
                "\n",
                "> Overall, I’m satisfied with this initial model and will now move on to try to improve it a little bit more. Just for fun.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Model Optimization"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "> In this step, I’m going to try improving the initial Linear Regression model by using **Ridge Regression**, which adds L2 regularization to reduce overfitting and improve generalization. \n",
                "- I’ll use `GridSearchCV` to tune the regularization strength parameter `alpha`.\n",
                "\n",
                "> This approach helps me find the best-performing version of Ridge Regression without manually testing multiple configurations. \n",
                "- Once I find the best model, I’ll compare its results to the original Linear Regression.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Best Alpha: 1\n",
                        "Mean Absolute Error (MAE): 2.3556\n",
                        "Mean Squared Error (MSE): 10.5771\n",
                        "R-squared (R²): 0.9921\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/vscode/.local/lib/python3.11/site-packages/scipy/_lib/_util.py:1226: LinAlgWarning: Ill-conditioned matrix (rcond=9.36095e-19): result may not be accurate.\n",
                        "  return f(*arrays, *other_args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.11/site-packages/scipy/_lib/_util.py:1226: LinAlgWarning: Ill-conditioned matrix (rcond=4.20484e-19): result may not be accurate.\n",
                        "  return f(*arrays, *other_args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.11/site-packages/scipy/_lib/_util.py:1226: LinAlgWarning: Ill-conditioned matrix (rcond=4.44712e-19): result may not be accurate.\n",
                        "  return f(*arrays, *other_args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.11/site-packages/scipy/_lib/_util.py:1226: LinAlgWarning: Ill-conditioned matrix (rcond=3.82288e-19): result may not be accurate.\n",
                        "  return f(*arrays, *other_args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.11/site-packages/scipy/_lib/_util.py:1226: LinAlgWarning: Ill-conditioned matrix (rcond=3.78097e-20): result may not be accurate.\n",
                        "  return f(*arrays, *other_args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.11/site-packages/scipy/_lib/_util.py:1226: LinAlgWarning: Ill-conditioned matrix (rcond=9.37815e-19): result may not be accurate.\n",
                        "  return f(*arrays, *other_args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.11/site-packages/scipy/_lib/_util.py:1226: LinAlgWarning: Ill-conditioned matrix (rcond=4.21244e-19): result may not be accurate.\n",
                        "  return f(*arrays, *other_args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.11/site-packages/scipy/_lib/_util.py:1226: LinAlgWarning: Ill-conditioned matrix (rcond=4.45527e-19): result may not be accurate.\n",
                        "  return f(*arrays, *other_args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.11/site-packages/scipy/_lib/_util.py:1226: LinAlgWarning: Ill-conditioned matrix (rcond=3.83017e-19): result may not be accurate.\n",
                        "  return f(*arrays, *other_args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.11/site-packages/scipy/_lib/_util.py:1226: LinAlgWarning: Ill-conditioned matrix (rcond=3.85702e-20): result may not be accurate.\n",
                        "  return f(*arrays, *other_args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.11/site-packages/scipy/_lib/_util.py:1226: LinAlgWarning: Ill-conditioned matrix (rcond=9.55023e-19): result may not be accurate.\n",
                        "  return f(*arrays, *other_args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.11/site-packages/scipy/_lib/_util.py:1226: LinAlgWarning: Ill-conditioned matrix (rcond=4.28848e-19): result may not be accurate.\n",
                        "  return f(*arrays, *other_args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.11/site-packages/scipy/_lib/_util.py:1226: LinAlgWarning: Ill-conditioned matrix (rcond=4.5368e-19): result may not be accurate.\n",
                        "  return f(*arrays, *other_args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.11/site-packages/scipy/_lib/_util.py:1226: LinAlgWarning: Ill-conditioned matrix (rcond=3.90313e-19): result may not be accurate.\n",
                        "  return f(*arrays, *other_args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.11/site-packages/scipy/_lib/_util.py:1226: LinAlgWarning: Ill-conditioned matrix (rcond=4.6181e-20): result may not be accurate.\n",
                        "  return f(*arrays, *other_args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.11/site-packages/scipy/_lib/_util.py:1226: LinAlgWarning: Ill-conditioned matrix (rcond=1.12745e-18): result may not be accurate.\n",
                        "  return f(*arrays, *other_args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.11/site-packages/scipy/_lib/_util.py:1226: LinAlgWarning: Ill-conditioned matrix (rcond=5.05031e-19): result may not be accurate.\n",
                        "  return f(*arrays, *other_args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.11/site-packages/scipy/_lib/_util.py:1226: LinAlgWarning: Ill-conditioned matrix (rcond=5.35366e-19): result may not be accurate.\n",
                        "  return f(*arrays, *other_args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.11/site-packages/scipy/_lib/_util.py:1226: LinAlgWarning: Ill-conditioned matrix (rcond=4.6342e-19): result may not be accurate.\n",
                        "  return f(*arrays, *other_args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.11/site-packages/scipy/_lib/_util.py:1226: LinAlgWarning: Ill-conditioned matrix (rcond=1.22772e-19): result may not be accurate.\n",
                        "  return f(*arrays, *other_args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.11/site-packages/scipy/_lib/_util.py:1226: LinAlgWarning: Ill-conditioned matrix (rcond=2.87924e-18): result may not be accurate.\n",
                        "  return f(*arrays, *other_args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.11/site-packages/scipy/_lib/_util.py:1226: LinAlgWarning: Ill-conditioned matrix (rcond=1.27859e-18): result may not be accurate.\n",
                        "  return f(*arrays, *other_args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.11/site-packages/scipy/_lib/_util.py:1226: LinAlgWarning: Ill-conditioned matrix (rcond=1.36491e-18): result may not be accurate.\n",
                        "  return f(*arrays, *other_args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.11/site-packages/scipy/_lib/_util.py:1226: LinAlgWarning: Ill-conditioned matrix (rcond=1.20608e-18): result may not be accurate.\n",
                        "  return f(*arrays, *other_args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.11/site-packages/scipy/_lib/_util.py:1226: LinAlgWarning: Ill-conditioned matrix (rcond=6.67519e-19): result may not be accurate.\n",
                        "  return f(*arrays, *other_args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.11/site-packages/scipy/_lib/_util.py:1226: LinAlgWarning: Ill-conditioned matrix (rcond=1.7735e-17): result may not be accurate.\n",
                        "  return f(*arrays, *other_args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.11/site-packages/scipy/_lib/_util.py:1226: LinAlgWarning: Ill-conditioned matrix (rcond=7.50162e-18): result may not be accurate.\n",
                        "  return f(*arrays, *other_args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.11/site-packages/scipy/_lib/_util.py:1226: LinAlgWarning: Ill-conditioned matrix (rcond=8.04055e-18): result may not be accurate.\n",
                        "  return f(*arrays, *other_args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.11/site-packages/scipy/_lib/_util.py:1226: LinAlgWarning: Ill-conditioned matrix (rcond=7.17705e-18): result may not be accurate.\n",
                        "  return f(*arrays, *other_args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.11/site-packages/scipy/_lib/_util.py:1226: LinAlgWarning: Ill-conditioned matrix (rcond=5.90609e-18): result may not be accurate.\n",
                        "  return f(*arrays, *other_args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.11/site-packages/scipy/_lib/_util.py:1226: LinAlgWarning: Ill-conditioned matrix (rcond=3.80907e-19): result may not be accurate.\n",
                        "  return f(*arrays, *other_args, **kwargs)\n"
                    ]
                }
            ],
            "source": [
                "# Define the parameter grid for Ridge\n",
                "param_grid = {'alpha': [0.01, 0.1, 1, 10, 100, 1000]}\n",
                "\n",
                "# Initialize Ridge model\n",
                "ridge = Ridge()\n",
                "\n",
                "# Use GridSearchCV to find best alpha\n",
                "grid = GridSearchCV(ridge, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
                "grid.fit(X_train, y_train)\n",
                "\n",
                "# Use best model to predict\n",
                "y_pred_ridge = grid.best_estimator_.predict(X_test)\n",
                "\n",
                "# Evaluate\n",
                "mae = mean_absolute_error(y_test, y_pred_ridge)\n",
                "mse = mean_squared_error(y_test, y_pred_ridge)\n",
                "r2 = r2_score(y_test, y_pred_ridge)\n",
                "\n",
                "print(\"Best Alpha:\", grid.best_params_['alpha'])\n",
                "print(\"Mean Absolute Error (MAE):\", round(mae, 4))\n",
                "print(\"Mean Squared Error (MSE):\", round(mse, 4))\n",
                "print(\"R-squared (R²):\", round(r2, 4))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "> I just optimized my model using Ridge Regression and cross-validation through `GridSearchCV`. The best alpha value turned out to be **1**, which gives just the right amount of regularization to help reduce overfitting.\n",
                "\n",
                "> The performance of this Ridge model is almost identical to my original Linear Regression model:\n",
                "\n",
                "- **MAE:** 2.3556  \n",
                "- **MSE:** 10.5771  \n",
                "- **R²:** 0.9921\n",
                "\n",
                "> This tells me that my initial model was already extremely accurate, and Ridge only made a tiny difference. That’s actually a good sign—it means the model is already good (it isnt)\n",
                "\n",
                "> I also got a few warnings (`LinAlgWarning: Ill-conditioned matrix`) during the cross-validation. These are related to numerical instability in the matrix inversion step, probably due to very small feature variances or strong feature correlations. \n",
                "- Since the performance is solid and consistent, I’m not too worried right now, but it’s something to keep in mind if I scale this up later.\n",
                "\n",
                "> (I dont believe that with only this information you can have such accurate predictions.)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Saving the model"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "> Now I'll save the model, so I can then proceed to prepare everything with Flask and then upload it to Render. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "['../models/ridge_model.pkl']"
                        ]
                    },
                    "execution_count": 16,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Ensure models directory exists\n",
                "os.makedirs('../models', exist_ok=True)\n",
                "\n",
                "# Save model and scaler\n",
                "joblib.dump(model, '../models/ridge_model.pkl')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "> Now let's go to app.py to configure the Flask code."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.4"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
